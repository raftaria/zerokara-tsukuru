{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('C:/myStudy/ZeroKaraTsukuru/deep-learning-from-scratch-master')\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|변수|설명|\n",
    "|--|--|\n",
    "|params|• 신경망의 매개변수를 보관하는 딕셔너리 변수(인스턴스 변수)<br>• params['W1']은 1번째 층의 가중치, params['b1']은 1번째 층의 편향|\n",
    "|grads|• 기울기를 보관하는 딕셔너리 변수(numerical_gradient() 메서드의 반환 값)<br>• grads['W1']은 첫 번쨰 층의 가중치의 기울기, grads['b1']은 1번째 층의 편향의 기울기|\n",
    "\n",
    "---\n",
    "|메서드|설명|\n",
    "|--|--|\n",
    "|\\__ init \\__(self, input_size,<br>hidden_size, output_size)|초기화를 수행한다.<br>인수는 순서대로 입력층의 뉴런 수, 은닉층의 뉴런 수, 출력층의 뉴런 수|\n",
    "|predict(x)|예측(추론)을 수행한다.<br>인수 x는 이미지 데이터|\n",
    "|loss(self, x, t)|손실함수의 값을 구한다.<br>인수 x는 이미지 데이터, t는 정답 레이블|\n",
    "|accuracy(self, x, t)|정확도를 구한다.|\n",
    "|numerical_gradient(self, x, t)|가중치 매개변수의 기울기를 구한다.|\n",
    "|gradient(self, x, t)|가중치 매개변수의 기울기를 구한다.<br>numerical_gradient()의 성능 개선판|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "\n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "\n",
    "        batch_num = x.shape[0]\n",
    "\n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "\n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, b1)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "\n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "\n",
    "print(net.params['W1'].shape)\n",
    "print(net.params['b1'].shape)\n",
    "print(net.params['W2'].shape)\n",
    "print(net.params['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09496862 0.09419766 0.09499724 0.10239416 0.10421688 0.10285519\n",
      "  0.09932903 0.10442651 0.09979416 0.10282055]\n",
      " [0.09500509 0.09392375 0.09490394 0.10244174 0.10422773 0.10279949\n",
      "  0.0993802  0.10463703 0.09957933 0.1031017 ]\n",
      " [0.0952609  0.09412494 0.09483403 0.10240385 0.10402732 0.10268943\n",
      "  0.09961748 0.10483685 0.09961249 0.10259272]\n",
      " [0.09524618 0.09395565 0.09504555 0.10250047 0.10395416 0.10305639\n",
      "  0.09947914 0.10439835 0.09991598 0.10244813]\n",
      " [0.09533587 0.09397313 0.09480702 0.10232798 0.10403018 0.10292264\n",
      "  0.09914702 0.10472218 0.10008913 0.10264486]\n",
      " [0.0954019  0.09388852 0.09483715 0.10239893 0.10405286 0.10293437\n",
      "  0.09948846 0.10471109 0.09973691 0.1025498 ]\n",
      " [0.09532988 0.09374457 0.09455597 0.10242748 0.10386316 0.10297687\n",
      "  0.09962231 0.10471644 0.10001989 0.10274343]\n",
      " [0.0952367  0.09446798 0.09485073 0.10235793 0.10395538 0.10297279\n",
      "  0.09946762 0.10460542 0.09969444 0.10239101]\n",
      " [0.09499538 0.09411498 0.09460816 0.10273639 0.10410342 0.10296956\n",
      "  0.09937835 0.10471499 0.0998065  0.10257229]\n",
      " [0.09523278 0.09425047 0.09463754 0.10252837 0.10404907 0.10278078\n",
      "  0.09917319 0.10464151 0.09987106 0.10283523]\n",
      " [0.09528949 0.09398595 0.09473427 0.10248673 0.10396338 0.10309374\n",
      "  0.09916541 0.10494186 0.09956696 0.1027722 ]\n",
      " [0.09561508 0.09433549 0.09485185 0.10236153 0.1040523  0.10259941\n",
      "  0.09907587 0.10418889 0.09998358 0.102936  ]\n",
      " [0.09521474 0.09405276 0.09507949 0.10215244 0.10387185 0.10286736\n",
      "  0.09957485 0.10471043 0.09948341 0.10299267]\n",
      " [0.09526864 0.09400699 0.09486195 0.10239759 0.10435193 0.1030349\n",
      "  0.09932462 0.10424187 0.09995625 0.10255527]\n",
      " [0.09514909 0.09373318 0.09499775 0.10243461 0.10393482 0.10307289\n",
      "  0.09957333 0.10474958 0.09962953 0.10272522]\n",
      " [0.09536175 0.09410892 0.0945752  0.10243308 0.10399691 0.10294511\n",
      "  0.09935221 0.10493507 0.09956769 0.10272405]\n",
      " [0.09531022 0.09396087 0.09482911 0.10222524 0.10428074 0.10271133\n",
      "  0.09973141 0.10456917 0.09973316 0.10264874]\n",
      " [0.09515259 0.09385753 0.09459359 0.10235719 0.10429828 0.10309046\n",
      "  0.09929696 0.10465667 0.10000462 0.10269211]\n",
      " [0.09508757 0.09396437 0.09513397 0.10248634 0.10425839 0.10291395\n",
      "  0.09977989 0.10426532 0.0990598  0.1030504 ]\n",
      " [0.09523097 0.09400796 0.09460511 0.10243653 0.10430949 0.10309394\n",
      "  0.09913706 0.10484492 0.0999234  0.10241063]\n",
      " [0.09526539 0.09402241 0.09466332 0.1024651  0.10374199 0.1031908\n",
      "  0.09939882 0.10480854 0.09971544 0.10272819]\n",
      " [0.09537318 0.09395442 0.09450358 0.10221455 0.10446372 0.10275245\n",
      "  0.09951252 0.10465871 0.09968084 0.10288603]\n",
      " [0.09516652 0.09379521 0.09503759 0.1021723  0.10385402 0.10307427\n",
      "  0.09945399 0.10480164 0.09986849 0.10277598]\n",
      " [0.09531526 0.09410972 0.09499605 0.10235161 0.10392326 0.10309005\n",
      "  0.09964846 0.10457574 0.09941554 0.1025743 ]\n",
      " [0.09552873 0.09427032 0.09497875 0.10235889 0.10376277 0.10290418\n",
      "  0.09931556 0.10429583 0.09964876 0.10293621]\n",
      " [0.09505609 0.09406489 0.09488482 0.10235548 0.10430997 0.10305314\n",
      "  0.09949161 0.10491433 0.09970891 0.10216077]\n",
      " [0.09540793 0.09406631 0.09492627 0.10233153 0.1037324  0.10283509\n",
      "  0.09949373 0.10467642 0.09968338 0.10284693]\n",
      " [0.09500065 0.09391937 0.09465364 0.10265831 0.10418341 0.10290437\n",
      "  0.09995665 0.10437988 0.09968668 0.10265703]\n",
      " [0.09506408 0.09406092 0.09476006 0.10230767 0.10405122 0.10356583\n",
      "  0.09933995 0.10513851 0.09923393 0.10247783]\n",
      " [0.09517802 0.09404467 0.09500158 0.10220055 0.10400314 0.10307949\n",
      "  0.09943324 0.10468745 0.09969505 0.1026768 ]\n",
      " [0.09546638 0.09393903 0.09484098 0.10224173 0.10397455 0.10299306\n",
      "  0.09957473 0.10451646 0.09956046 0.10289261]\n",
      " [0.09524492 0.09392103 0.09483392 0.10220678 0.10400555 0.10313182\n",
      "  0.09946316 0.10473043 0.09973664 0.10272575]\n",
      " [0.09499526 0.09428009 0.09489298 0.10229714 0.10358793 0.10340822\n",
      "  0.09953719 0.10498475 0.09936898 0.10264748]\n",
      " [0.09517077 0.0941897  0.09482029 0.10233246 0.10434053 0.10283692\n",
      "  0.09919258 0.10466254 0.09985817 0.10259604]\n",
      " [0.09514367 0.09401499 0.09479882 0.10220558 0.10407221 0.1030665\n",
      "  0.09974728 0.10460389 0.09944844 0.10289863]\n",
      " [0.09529154 0.09401984 0.09486037 0.10228077 0.10426709 0.10284471\n",
      "  0.09927363 0.10465472 0.09943917 0.10306816]\n",
      " [0.09518248 0.09419275 0.09497401 0.10242027 0.10401949 0.10273167\n",
      "  0.09955841 0.10455538 0.0996318  0.10273373]\n",
      " [0.09499633 0.09441387 0.09446179 0.10253117 0.10421933 0.10284247\n",
      "  0.09911308 0.1047244  0.09992216 0.10277541]\n",
      " [0.0951887  0.09426046 0.09528926 0.1021619  0.10372283 0.10335401\n",
      "  0.09942867 0.10439142 0.09931588 0.10288686]\n",
      " [0.09547064 0.09390177 0.09499094 0.10259205 0.10390804 0.10282383\n",
      "  0.09935391 0.10438637 0.09978629 0.10278615]\n",
      " [0.09509979 0.09404192 0.09491933 0.10228736 0.10394574 0.10308956\n",
      "  0.09957372 0.10466926 0.09994162 0.10243171]\n",
      " [0.09539071 0.09358726 0.0949778  0.10221522 0.10426933 0.10286535\n",
      "  0.09915497 0.10466566 0.10007928 0.10279442]\n",
      " [0.09496223 0.09459651 0.09480243 0.10232558 0.10358161 0.10309844\n",
      "  0.09946051 0.10482954 0.09950925 0.10283389]\n",
      " [0.09516005 0.09419184 0.09476257 0.10222094 0.10378519 0.10294828\n",
      "  0.09918477 0.10490006 0.09988424 0.10296206]\n",
      " [0.09537129 0.09367273 0.09500442 0.10255772 0.10423276 0.10283571\n",
      "  0.09972833 0.10435566 0.09980576 0.10243562]\n",
      " [0.09491616 0.0939654  0.09468417 0.1022496  0.10442209 0.1030409\n",
      "  0.09928169 0.10488523 0.099792   0.10276276]\n",
      " [0.09530494 0.09413712 0.09489667 0.10222693 0.10427744 0.10295511\n",
      "  0.0997365  0.10439527 0.09956661 0.1025034 ]\n",
      " [0.09503833 0.09399468 0.09470458 0.10235154 0.10426836 0.1027862\n",
      "  0.09925242 0.10489035 0.09998883 0.10272472]\n",
      " [0.09511576 0.09422349 0.0948615  0.10235094 0.10435688 0.1028363\n",
      "  0.09931881 0.10447952 0.09966699 0.10278981]\n",
      " [0.09539486 0.09391589 0.09488407 0.10242356 0.10416131 0.10301642\n",
      "  0.09970854 0.1046043  0.09926191 0.10262916]\n",
      " [0.09550515 0.09413357 0.09470229 0.10264981 0.10435898 0.10263723\n",
      "  0.09921855 0.10461785 0.09946072 0.10271584]\n",
      " [0.09534972 0.09398179 0.0950491  0.10211669 0.10406047 0.10291889\n",
      "  0.0993236  0.10471718 0.09976541 0.10271716]\n",
      " [0.09523558 0.09393266 0.09480069 0.10233076 0.10412102 0.10300415\n",
      "  0.09972103 0.10450113 0.09959864 0.10275435]\n",
      " [0.09499882 0.09401727 0.09512123 0.10207518 0.10374015 0.10338508\n",
      "  0.09971799 0.1047457  0.09963083 0.10256774]\n",
      " [0.09556576 0.09422469 0.09482497 0.10243955 0.10416497 0.1024057\n",
      "  0.09928576 0.10422811 0.09982538 0.10303512]\n",
      " [0.09529947 0.09421202 0.0948257  0.10233111 0.10379124 0.10319385\n",
      "  0.09967421 0.10452118 0.09973195 0.10241928]\n",
      " [0.09494169 0.09412345 0.09463026 0.10213229 0.10455668 0.10273975\n",
      "  0.09938493 0.10471187 0.09978391 0.10299518]\n",
      " [0.09537976 0.09391151 0.09441969 0.10251465 0.10412391 0.10305895\n",
      "  0.09946686 0.10453448 0.09976591 0.10282429]\n",
      " [0.09499692 0.09399644 0.09486005 0.10241703 0.10439181 0.10317233\n",
      "  0.09889166 0.10482461 0.09980364 0.10264551]\n",
      " [0.09560992 0.09386244 0.0947302  0.10245105 0.10398226 0.10315815\n",
      "  0.09939464 0.10464242 0.09959012 0.1025788 ]\n",
      " [0.09525561 0.09405293 0.09492314 0.1022774  0.1039132  0.10304573\n",
      "  0.09945742 0.10491499 0.09943476 0.10272482]\n",
      " [0.09511408 0.09410178 0.09504081 0.10249069 0.10404588 0.10268135\n",
      "  0.09923299 0.10465984 0.09980958 0.102823  ]\n",
      " [0.09524358 0.09420725 0.09462885 0.10214292 0.10412677 0.10323725\n",
      "  0.09932866 0.10474126 0.09983522 0.10250824]\n",
      " [0.0952161  0.09397473 0.09487834 0.10260959 0.1040625  0.10293607\n",
      "  0.09917842 0.10485222 0.09976444 0.10252759]\n",
      " [0.0952618  0.09390295 0.09467396 0.10250242 0.10404599 0.10291487\n",
      "  0.09936888 0.10464303 0.09992676 0.10275934]\n",
      " [0.0955826  0.09408524 0.09460209 0.10227284 0.10421559 0.10261486\n",
      "  0.09923532 0.10461403 0.09985341 0.10292402]\n",
      " [0.09512608 0.09397536 0.09497727 0.1024369  0.10413716 0.10315119\n",
      "  0.09942409 0.10435542 0.09963979 0.10277674]\n",
      " [0.09521269 0.09416359 0.09462104 0.10234613 0.10402992 0.10285146\n",
      "  0.09917    0.10505616 0.09987518 0.10267383]\n",
      " [0.09542319 0.09415836 0.09470946 0.10255548 0.10392918 0.10306443\n",
      "  0.09933779 0.10455404 0.09990538 0.1023627 ]\n",
      " [0.09519829 0.09396237 0.094793   0.10239685 0.1040341  0.10315453\n",
      "  0.09945834 0.10452006 0.0997351  0.10274735]\n",
      " [0.09541554 0.09405076 0.09478519 0.10219253 0.10371463 0.10291573\n",
      "  0.09963584 0.10509953 0.09963034 0.10255991]\n",
      " [0.09541965 0.09412478 0.09456143 0.10209166 0.10440066 0.10262035\n",
      "  0.0995654  0.10451923 0.09988484 0.10281201]\n",
      " [0.09526806 0.09412498 0.09470899 0.10251957 0.10430884 0.1024872\n",
      "  0.09966255 0.10403871 0.09972361 0.10315749]\n",
      " [0.09541419 0.09408058 0.09479283 0.10240094 0.10386431 0.10299975\n",
      "  0.09934314 0.10453052 0.09968094 0.1028928 ]\n",
      " [0.09517873 0.09435191 0.09507528 0.10238518 0.10409743 0.10281688\n",
      "  0.09962861 0.10426877 0.09959178 0.10260545]\n",
      " [0.09497659 0.09423661 0.09517271 0.10216436 0.10383228 0.10307385\n",
      "  0.09971454 0.10456066 0.09957743 0.10269097]\n",
      " [0.09535095 0.09400226 0.09466055 0.10228945 0.10389408 0.10316501\n",
      "  0.09922856 0.10522888 0.09957859 0.10260166]\n",
      " [0.09522724 0.09416607 0.09462617 0.10261524 0.10434103 0.10261475\n",
      "  0.0990328  0.10449445 0.09998508 0.10289717]\n",
      " [0.09545137 0.09414127 0.09489844 0.1021496  0.10384169 0.10282967\n",
      "  0.09951903 0.10489475 0.09962044 0.10265373]\n",
      " [0.09530828 0.09457191 0.09491734 0.10209185 0.10363565 0.10319754\n",
      "  0.09962576 0.10448573 0.09944794 0.102718  ]\n",
      " [0.09547227 0.09382265 0.09516272 0.10234359 0.10422525 0.10270732\n",
      "  0.09925069 0.10470918 0.09969525 0.10261108]\n",
      " [0.09513903 0.09385445 0.09481353 0.10229971 0.10412304 0.10310963\n",
      "  0.09919541 0.10453124 0.10011795 0.10281601]\n",
      " [0.09531013 0.0938009  0.09435226 0.10259349 0.10409288 0.10309464\n",
      "  0.0990962  0.10482082 0.10024388 0.10259481]\n",
      " [0.09524203 0.09379989 0.09501293 0.10231206 0.10381151 0.10309591\n",
      "  0.09945865 0.10459433 0.09944987 0.10322282]\n",
      " [0.0950354  0.09415447 0.09464406 0.10248422 0.10418026 0.10309001\n",
      "  0.09958087 0.10464059 0.09945183 0.10273829]\n",
      " [0.09520471 0.09398924 0.09500374 0.10232223 0.10384366 0.10298923\n",
      "  0.09942097 0.10481934 0.09978812 0.10261875]\n",
      " [0.0954197  0.09415543 0.09466012 0.10217695 0.10435185 0.10292381\n",
      "  0.09951721 0.1045318  0.0995623  0.10270082]\n",
      " [0.09519804 0.09426736 0.0946962  0.10234981 0.1042725  0.10266433\n",
      "  0.09920763 0.10458542 0.09960598 0.10315274]\n",
      " [0.09530578 0.09417979 0.09457456 0.10233966 0.10430938 0.10295016\n",
      "  0.0994445  0.10449887 0.09968051 0.10271678]\n",
      " [0.09498854 0.09421218 0.09484397 0.10229631 0.10433399 0.10304639\n",
      "  0.09930072 0.10465329 0.09960434 0.10272028]\n",
      " [0.09530247 0.09399273 0.09491774 0.10233424 0.10401336 0.10292134\n",
      "  0.09948522 0.1047462  0.09960982 0.10267688]\n",
      " [0.09520576 0.09427937 0.09483328 0.10214987 0.10419837 0.10314526\n",
      "  0.09941264 0.10496416 0.09980547 0.10200582]\n",
      " [0.09563228 0.09380614 0.09489154 0.10198175 0.1043046  0.10268302\n",
      "  0.09930893 0.10440381 0.09987396 0.10311398]\n",
      " [0.09519163 0.09414934 0.0948493  0.10219886 0.10422426 0.10291961\n",
      "  0.0995225  0.10452831 0.09959038 0.10282581]\n",
      " [0.09515011 0.09406343 0.09503713 0.10235147 0.10400813 0.10297191\n",
      "  0.0994364  0.10454423 0.09948612 0.10295105]\n",
      " [0.09533341 0.09439454 0.09483774 0.1021091  0.10407658 0.10299677\n",
      "  0.09953929 0.10437286 0.09978348 0.10255622]\n",
      " [0.09522046 0.0939038  0.09484829 0.1022673  0.10409516 0.10307356\n",
      "  0.099662   0.10453516 0.09979296 0.10260129]\n",
      " [0.09533479 0.09401717 0.09476201 0.10217821 0.10419986 0.10319086\n",
      "  0.09963988 0.10390907 0.09973098 0.10303717]\n",
      " [0.095144   0.09431547 0.09474662 0.10255755 0.1041578  0.10297967\n",
      "  0.09909571 0.10483941 0.09992634 0.10223742]\n",
      " [0.09495074 0.09414709 0.09493045 0.1026937  0.10402969 0.10301233\n",
      "  0.09902885 0.10496151 0.09985976 0.10238587]]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(100, 784)\n",
    "y = net.predict(x)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
